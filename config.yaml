audio:
  sample_rate: 16000
  chunk_seconds: 6
  device: "MacBook Pro Microphone"        # null = default input; or e.g. 1 for a specific device
  min_words_per_push: 5  # only trigger predictions if at least this many words in latest chunk

whisper:
  model_size: small      # tiny / base / small / medium / large-v3
  compute_type: int8_float16  # float16 on GPU; use int8 for CPU-only
  vad_filter: true

context:
  max_chars: 12000       # sliding window of transcript for ranking
  tail_chars: 8000       # how much of newest text to keep when trimming

bible:
  xml_paths:
    - ./bibles           # folder with many XMLs (searched recursively)
  index_path: ./bible_index.faiss
  meta_path:  ./bible_meta.json
  encoder: sentence-transformers/all-MiniLM-L6-v2
  top_k_candidates: 40

llm:
  provider: ollama                 # ollama | openai
  model: deepseek-r1:14b             # e.g. deepseek-r1:7b, llama3.1:8b, qwen2.5:7b
  openai_model: gpt-4o-mini
  endpoint: http://127.0.0.1:11434 # Ollama
  temperature: 0.2

output:
  write_file: ./predictions.json
  pretty_print: true
  print_to_console: true